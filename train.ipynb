{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from data_loader import data_load\n",
    "from IPython.display import clear_output\n",
    "from model.model import fcn_decoder,vgg16_encoder\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "IMG_WIDTH = 64\n",
    "IMG_HEIGHT = 64\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "       gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5500)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "encoder = vgg16_encoder((IMG_HEIGHT, IMG_WIDTH,3))\n",
    "decoder_src = fcn_decoder((IMG_HEIGHT, IMG_WIDTH,3), encoder)\n",
    "decoder_dst = fcn_decoder((IMG_HEIGHT, IMG_WIDTH,3), encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset, dst_dataset = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "dst_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "src_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "dst_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "src_log_dir = 'logs/fcn_log/gradient_tape/' + current_time + '/src'\n",
    "dst_log_dir = 'logs/fcn_log/gradient_tape/' + current_time + '/dst'\n",
    "src_summary_writer = tf.summary.create_file_writer(src_log_dir)\n",
    "dst_summary_writer = tf.summary.create_file_writer(dst_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(flag):\n",
    "    \n",
    "    img_idx = [0, 100, 200, 300]\n",
    "    src = []\n",
    "    dst = []\n",
    "    \n",
    "    for i in img_idx:\n",
    "        src += [(img, land) for img, land in src_dataset.skip(i).take(1)] \n",
    "        dst += [(img, land) for img, land in dst_dataset.skip(i).take(1)] \n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    if flag ==\"src\":\n",
    "        for i in range(4):\n",
    "            plt.subplot(4,4, 4*i +1)\n",
    "            plt.imshow(src[i][1])\n",
    "            plt.axis('off')\n",
    "            plt.title(\"src landmark\")\n",
    "            \n",
    "            plt.subplot(4,4, 4*i +2)\n",
    "            plt.imshow(decoder_src(src[i][1][tf.newaxis,...])[0])  \n",
    "            plt.title(\"pred_src\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(4,4, 4*i +3)\n",
    "            plt.imshow(decoder_dst(src[i][1][tf.newaxis,...])[0])  \n",
    "            plt.title(\"pred_dst\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(4,4, 4*i +4)\n",
    "            plt.imshow(src[i][0])   \n",
    "            plt.title(\"src image\")\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        for i in range(4):\n",
    "            plt.subplot(4,4, 4*i +1)\n",
    "            plt.imshow(dst[i][1])\n",
    "            plt.axis('off')\n",
    "            plt.title(\"dst landmark\")\n",
    "            \n",
    "            plt.subplot(4,4, 4*i +2)\n",
    "            plt.imshow(decoder_dst(dst[i][1][tf.newaxis,...])[0])  \n",
    "            plt.axis('off')\n",
    "            plt.title(\"pred_dst\")\n",
    "\n",
    "            plt.subplot(4,4, 4*i +3)\n",
    "            plt.imshow(decoder_src(dst[i][1][tf.newaxis,...])[0])  \n",
    "            plt.title(\"pred_src\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(4,4, 4*i +4)\n",
    "            plt.imshow(dst[i][0])   \n",
    "            plt.title(\"dst image\")\n",
    "            plt.axis('off') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_predictions(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def src_train_step(src_img, src_land):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_src = decoder_src(src_land)\n",
    "        \n",
    "        src_batch_loss =  tf.keras.losses.MSE(pred_src, src_img)\n",
    "\n",
    "    gradients_of_src_model = tape.gradient(src_batch_loss, decoder_src.trainable_variables)\n",
    "\n",
    "    src_optimizer.apply_gradients(zip(gradients_of_src_model, decoder_src.trainable_variables))\n",
    "\n",
    "    src_loss(src_batch_loss)\n",
    "\n",
    "@tf.function\n",
    "def dst_train_step(dst_img, dst_land):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_dst = decoder_dst(dst_land)\n",
    "        \n",
    "        dst_batch_loss =  tf.keras.losses.MSE(pred_dst, dst_img)\n",
    "\n",
    "    gradients_of_dst_model = tape.gradient(dst_batch_loss, decoder_dst.trainable_variables)\n",
    "\n",
    "    dst_optimizer.apply_gradients(zip(gradients_of_dst_model, decoder_dst.trainable_variables))\n",
    "\n",
    "    dst_loss(dst_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(src_dataset, dst_dataset, epochs, batch_size):\n",
    "    \n",
    "    src_best_loss =0.01\n",
    "    dst_best_loss =0.01\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if epoch >=100:\n",
    "            src_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "            dst_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "\n",
    "        if epoch >=1000:\n",
    "            src_optimizer = tf.keras.optimizers.Adam(1e-6)\n",
    "            dst_optimizer = tf.keras.optimizers.Adam(1e-6)\n",
    "\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        src_loss.reset_states()\n",
    "        dst_loss.reset_states()\n",
    "      \n",
    "        \n",
    "        batch_time = time.time()\n",
    "        batch_idx = 0\n",
    "        \n",
    "        for image_batch, land_batch in src_dataset.batch(batch_size):\n",
    "                        \n",
    "            '''if tf.random.uniform(()) > 0.5:\n",
    "                image_batch = tf.image.flip_left_right(image_batch)\n",
    "                label_batch = tf.image.flip_left_right(land_batch)'''\n",
    "\n",
    "            \n",
    "            src_train_step(image_batch, land_batch)\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        #show_predictions(\"src\")      \n",
    "                \n",
    "                \n",
    "        print('epoch {}, loss {} , time {}'.format(epoch+1, src_loss.result(), time.time()- batch_time))\n",
    "\n",
    "        \n",
    "        for image_batch, land_batch in dst_dataset.batch(batch_size):\n",
    "                        \n",
    "            '''if tf.random.uniform(()) > 0.5:\n",
    "                \n",
    "                image_batch = tf.image.flip_left_right(image_batch)\n",
    "                label_batch = tf.image.flip_left_right(land_batch)'''\n",
    "\n",
    "            \n",
    "            dst_train_step(image_batch, land_batch)\n",
    "            \n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        #show_predictions(\"dst\")      \n",
    "        \n",
    "        \n",
    "        print('epoch {}, loss {} , time {}'.format(epoch+1, dst_loss.result(), time.time()- batch_time))\n",
    "        \n",
    "        if src_best_loss > src_loss.result().numpy():\n",
    "            src_path = \"model_h5/fcn/src3.h5\" \n",
    "            src_best_loss =src_loss.result().numpy()\n",
    "            decoder_src.save(src_path)\n",
    "\n",
    "        if dst_best_loss > dst_loss.result().numpy():\n",
    "            dst_path = \"model_h5/fcn/dst3.h5\" \n",
    "            dst_best_loss =dst_loss.result().numpy()\n",
    "            decoder_dst.save(dst_path)\n",
    "\n",
    "            \n",
    "        with src_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', src_loss.result(), step=epoch+1)\n",
    "                \n",
    "        with dst_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', dst_loss.result(), step=epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(src_dataset, dst_dataset, 3000, 300) # you have to modify batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(\"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(\"src\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
